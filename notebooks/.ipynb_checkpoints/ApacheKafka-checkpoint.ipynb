{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "77ca9a50-7c3b-42aa-8a3e-343d9b021e3f",
   "metadata": {},
   "source": [
    "# Apache Kafka\n",
    "Powering Real-Time Data at Scale\n",
    "\n",
    "![Apache Kafka](images/logo.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12b18030-cadc-48f2-9a4d-b3c58e855f2d",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "This workshop aims to provide a practical understanding of Apache Kafka's core concepts and hands-on experience with its functionality.\n",
    "\n",
    "### Objectives\n",
    "- Understand the key concepts of Event Streaming and Apache Kafka.\n",
    "- Learn how to produce and consume messages.\n",
    "- Explore advanced features like partitions and fault tolerance.\n",
    "- Conclude with a quiz to summarize and memorize \n",
    "\n",
    "[Apache Kafka Official Documentation](https://kafka.apache.org/documentation/)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5663f2fc-3da2-41c0-99f0-c3aa1bbd4379",
   "metadata": {},
   "source": [
    "# Introduction to Event Streaming\n",
    "\n",
    "Event streaming is a technology paradigm for continuously capturing, storing, processing, and reacting to events happening across your business or applications in real time. It fundamentally changes how data is handled, moving away from batch processing to a continual flow of data.\n",
    "\n",
    "Event streaming platforms like Apache Kafka enable the collection, integration, and analysis of massive streams of event data from multiple sources.\n",
    "\n",
    "![Event Streaming](images/event.svg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "456ce34f-6827-40c0-9ea1-5adb3bd598e2",
   "metadata": {},
   "source": [
    "## The Relevance of Event Streaming\n",
    "\n",
    "Event streaming has become vital in today’s digital world, where real-time data and insights are crucial for decision making. Applications range from real-time analytics and monitoring to data integration and microservices communication.\n",
    "\n",
    "### <span style=\"color:red\">Question: Why do you think real-time data processing is important in modern applications?</span>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e64c67fd-9452-4b02-9bb4-6dca693b1c4a",
   "metadata": {},
   "source": [
    "## What is Apache Kafka?\n",
    "\n",
    "Apache Kafka is a distributed event streaming platform that provides high-throughput, highly scalable, and fault-tolerant event streaming capabilities. It is designed to handle real-time data feeds and provides a unified platform for both producing and consuming data streams.\n",
    "\n",
    "### Key Features:\n",
    "- **High Throughput**: Capable of handling millions of events per second.\n",
    "- **Scalability**: Easily scales horizontally to accommodate growing data.\n",
    "- **Fault Tolerance**: Robust against system failures, ensuring no data loss.\n",
    "- **Real-Time Processing**: Enables immediate data processing and decision making.\n",
    "\n",
    "![Applications](images/apps.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "821409c1-d428-4b20-93f6-b548f90eea32",
   "metadata": {},
   "source": [
    "# Companies Using Apache Kafka\n",
    "\n",
    "Apache Kafka is widely adopted by numerous companies across various industries. Its ability to handle large-scale, real-time data makes it a preferred choice for modern data architectures.\n",
    "\n",
    "## Some Notable Companies:\n",
    "- **LinkedIn**: Originally developed Kafka to handle their activity stream and operational metrics.\n",
    "- **Netflix**: Utilizes Kafka for real-time monitoring and event processing in their streaming service.\n",
    "- **Uber**: Employs Kafka for gathering user, trip, and geospatial data for real-time analytics and decision-making.\n",
    "- **Twitter**: Uses Kafka as a backbone for their event streaming architecture, handling billions of events each day.\n",
    "\n",
    "### <span style=\"color:red\">Question: Can you think of a scenario in your industry or field where Kafka's capabilities would be beneficial?</span>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05cd00a6-c892-47fc-a2de-5d5034a9595d",
   "metadata": {},
   "source": [
    "# Transition to Kafka Key Concepts\n",
    "\n",
    "Now that we understand the importance of event streaming and the role of Apache Kafka in this domain, let's delve into the key concepts that make Kafka a powerful tool for event-driven data processing.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce771c06-5afc-41f6-bd70-02f2c900c789",
   "metadata": {},
   "source": [
    "## Overview of Apache Kafka\n",
    "\n",
    "### Key Concepts\n",
    "\n",
    "Understanding the fundamental concepts of Apache Kafka is crucial for working with this powerful streaming platform. In this section, we will explore the essential components and their roles in Kafka's architecture.\n",
    "\n",
    "- **Topics**: Categories where records are stored.\n",
    "- **Producers**: Entities that publish messages to topics.\n",
    "- **Consumers**: Entities that subscribe to topics and process messages.\n",
    "- **Brokers**: Servers in a Kafka cluster that store data and serve clients.  \n",
    "- **Partitions**: Kafka topics are divided into partitions, which allow for data to be distributed and parallelized across multiple brokers.\n",
    "- **Offsets**: Unique identifiers of records within a partition.\n",
    "\n",
    "![Kafka Architecture](images/simple.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96a82566-bd84-4a93-a1b7-e86646a587e8",
   "metadata": {},
   "source": [
    "## 1. Topics\n",
    "\n",
    "- **Definition**: A topic is a category or feed name to which records are published. It is like a channel where data is stored and distributed.\n",
    "- **Characteristics**:\n",
    "  - Topics are partitioned for scalability.\n",
    "  - Data within a topic is immutable.\n",
    "- **Use Case**: Different topics for logs, metrics, customer activities, etc.\n",
    "\n",
    "### <span style=\"color:red\">Question: What purpose do partitions within a topic serve in Apache Kafka?</span>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6da3528-dba0-4e6f-a1f0-7aed4d5cdd32",
   "metadata": {},
   "source": [
    "## 2. Producers\n",
    "\n",
    "- **Definition**: Applications or processes that publish data to Kafka topics.\n",
    "- **How it Works**:\n",
    "  - Producers send data to topics, optionally choosing the partition.\n",
    "  - Data can be sent synchronously or asynchronously.\n",
    "- **Key Points**: Responsible for key determination and efficient data distribution.\n",
    "\n",
    "### <span style=\"color:red\">Question: How do producers influence which partition a message is sent to in Kafka?</span>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff555dcb-6951-499b-8024-5feb7e08fdfc",
   "metadata": {},
   "source": [
    "## 3. Consumers\n",
    "\n",
    "- **Definition**: Processes that read data from Kafka topics.\n",
    "- **Consumption Patterns**:\n",
    "  - Subscribe to one or more topics and read data in order.\n",
    "  - Track which records have been consumed using offsets.\n",
    "- **Grouping**: Part of a consumer group to avoid duplicate processing.\n",
    "\n",
    "### <span style=\"color:red\">Question: Why is it important for consumers to track offsets in Kafka?</span>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c749f3c-0057-4437-8394-91161a66bc64",
   "metadata": {},
   "source": [
    "![Topic](images/topic.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6956d276-ad90-4ea3-941c-b0ff07cba804",
   "metadata": {},
   "source": [
    "![Producers](images/producers.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9058159-e14c-42be-bc17-0f3997c69c13",
   "metadata": {},
   "source": [
    "## 4. Brokers\n",
    "\n",
    "- **Definition**: Servers in a Kafka cluster storing data and serving clients.\n",
    "- **Cluster Role**:\n",
    "  - Handle load balancing and fault tolerance.\n",
    "  - Manage requests from producers and serve data to consumers.\n",
    "- **Replication**: Ensures data availability and durability.\n",
    "\n",
    "### <span style=\"color:red\">Question: What is the role of a broker in the Kafka architecture?</span>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b9a796e-c925-4d49-8966-c583d69c187a",
   "metadata": {},
   "source": [
    "## 5. Partitions\n",
    "\n",
    "- **Definition**: Divisions within a Kafka topic.\n",
    "- **Scaling and Performance**:\n",
    "  - Enable parallel processing across nodes.\n",
    "  - Hosted on different servers for better data handling and consumer management.\n",
    "\n",
    "### <span style=\"color:red\">Question: How do partitions contribute to Kafka’s scalability and fault tolerance?</span>\n",
    "\n",
    "![Partitions](images/partitions.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee3bb295-fefc-4203-b846-3578ea271aa4",
   "metadata": {},
   "source": [
    "![More Consumers](images/more_consumers.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad16ffce-3be2-4e6a-9769-c606bcd23f7f",
   "metadata": {},
   "source": [
    "![Impossible](images/impossible.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3eb498eb-df40-43ad-891d-82a39ff8741e",
   "metadata": {},
   "source": [
    "![Idle Consumer](images/idle_consumer.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50b9d669-e29b-4010-9558-4a20abbf243a",
   "metadata": {},
   "source": [
    "![More Groups](images/more_groups.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c25b5643-99bb-4677-9f0d-54a0b8234210",
   "metadata": {},
   "source": [
    "## 6. Offsets\n",
    "\n",
    "- **Definition**: Unique identifiers for each record within a partition.\n",
    "- **Consumer Tracking**:\n",
    "  - Enable consumers to keep track of consumed messages.\n",
    "  - Allow resuming reading from the last consumed offset.\n",
    "\n",
    "### <span style=\"color:red\">Question: What would happen if Kafka consumers didn't use offsets?</span>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bab2152-6179-4cd7-935b-0d91a5e4a8e0",
   "metadata": {},
   "source": [
    "## Kafka's Transition from ZooKeeper to KRaft\n",
    "\n",
    "### ZooKeeper in Kafka\n",
    "\n",
    "- **Role**: ZooKeeper has been integral to Kafka for tasks like controller election, cluster membership, topic configuration, ACLs, and quotas.\n",
    "\n",
    "- **Challenges**:\n",
    "\n",
    "  - **Scalability**: ZooKeeper can become a bottleneck in large Kafka clusters.\n",
    "\n",
    "  - **Complexity**: Adds a layer of complexity and dependency to Kafka’s architecture.\n",
    "\n",
    "  - **Consistency**: Requires a quorum of nodes to be available to process any request.\n",
    "\n",
    "![Zookeeper](images/zookeeper.png)\n",
    "\n",
    "### Introduction to KRaft\n",
    "\n",
    "- **KRaft** (Kafka Raft Metadata mode) is Kafka's new consensus protocol, replacing ZooKeeper.\n",
    "\n",
    "- **Benefits**:\n",
    "\n",
    "  - **Simplicity**: Simplifies Kafka’s architecture by removing the need for ZooKeeper.\n",
    "\n",
    "  - **Scalability**: Improves scalability by reducing the load on the metadata store.\n",
    "\n",
    "  - **Availability**: Enhances Kafka’s availability by allowing partial failures in the metadata store.\n",
    "\n",
    "### Transition from ZooKeeper to KRaft\n",
    "\n",
    "- Kafka's architecture overhaul involves moving from ZooKeeper to KRaft, starting with Kafka 2.8.\n",
    "\n",
    "- **Considerations**:\n",
    "\n",
    "  - **Compatibility**: Transition requires careful planning and testing due to compatibility issues.\n",
    "\n",
    "  - **Consistency**: KRaft allows eventual consistency in some cases.\n",
    "\n",
    "![ZooKeeper vs. KRaft](images/kraft.jpg)\n",
    "\n",
    "### <span style=\"color:red\">Question</span>\n",
    "\n",
    "**What major challenge associated with ZooKeeper was addressed by Kafka's transition to KRaft?**\n",
    "\n",
    "- A) Inability to handle large data volumes.\n",
    "\n",
    "- B) Complexity and scalability issues.\n",
    "\n",
    "- C) Lack of security features.\n",
    "\n",
    "- D) Difficulty in integrating with other systems.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0abe8c0b-eaf4-4447-8de0-ffdc8fe1e475",
   "metadata": {},
   "source": [
    "# Key Concepts Conclusion\n",
    "\n",
    "These key concepts form the backbone of Apache Kafka's architecture and functionality. Understanding these will aid in grasping more advanced topics and effectively utilizing Kafka for real-time data processing.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ebd7fc5-ad47-426c-ac44-2294449243c6",
   "metadata": {},
   "source": [
    "## Exercises\n",
    "\n",
    "## Exercise 1: Basic Messaging with Kafka\n",
    "\n",
    "### Objective\n",
    "Understand the fundamental process of producing and consuming messages using Kafka.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d36dba02-f8cd-4311-8240-ab5ebc30a139",
   "metadata": {},
   "source": [
    "### Producing and Consuming Messages\n",
    "\n",
    "#### Setup\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2def66e5-d29d-4c1b-89fc-ae5d8b086443",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: confluent-kafka in /opt/conda/lib/python3.11/site-packages (2.3.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install --no-cache-dir confluent-kafka\n",
    "\n",
    "from confluent_kafka import Producer, Consumer, KafkaException\n",
    "import json\n",
    "\n",
    "\n",
    "# Kafka Configuration\n",
    "kafka_brokers = \"kafka\"  # Use the Docker service name as the hostname\n",
    "topic_a = \"topic-a\"\n",
    "topic_b = \"topic-b\"\n",
    "topic_c = \"topic_c\"\n",
    "\n",
    "# Function for producer creation\n",
    "def create_producer(broker_url):\n",
    "    conf = {'bootstrap.servers': broker_url}\n",
    "    return Producer(conf)\n",
    "\n",
    "# Function for consumers creation\n",
    "def create_consumer(broker_url, group_id, auto_offset_reset='earliest'):\n",
    "    conf = {\n",
    "        'bootstrap.servers': broker_url,\n",
    "        'group.id': group_id,\n",
    "        'auto.offset.reset': auto_offset_reset\n",
    "    }\n",
    "    return Consumer(conf)\n",
    "\n",
    "# Function for error reporting\n",
    "def delivery_report(err, msg):\n",
    "    if err is not None:\n",
    "        print(f\"Message delivery failed: {err}\")\n",
    "    else:\n",
    "        print(f\"Message delivered to {msg.topic()} [{msg.partition()}]\")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "25f429ef-3b04-4924-94a4-0e067cf91c58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Consumer started\n"
     ]
    }
   ],
   "source": [
    "import threading\n",
    "import confluent_kafka\n",
    "import time\n",
    "\n",
    "\n",
    "# Define the on_assign callback function\n",
    "def on_assign(consumer, partitions):\n",
    "    for p in partitions:\n",
    "        # Reset the offset to the beginning\n",
    "        p.offset = confluent_kafka.OFFSET_BEGINNING\n",
    "    consumer.assign(partitions)\n",
    "\n",
    "# List to hold messages\n",
    "messages = []\n",
    "\n",
    "# Consumer function to run in a separate thread\n",
    "def consume_messages():\n",
    "    try:\n",
    "        while True:\n",
    "            msg = consumer.poll(1.0)\n",
    "            if msg is None:\n",
    "                continue\n",
    "            if msg.error():\n",
    "                print(f\"Consumer error: {msg.error()}\")\n",
    "                continue\n",
    "            print(f\"Received message: {msg.value().decode('utf-8')}\")\n",
    "            messages.append(msg.value().decode('utf-8'))\n",
    "    except KeyboardInterrupt:\n",
    "        pass\n",
    "    finally:\n",
    "        consumer.close()\n",
    "        \n",
    "# Initialize the consumer\n",
    "consumer = create_consumer(kafka_brokers, \"mygroup\")\n",
    "\n",
    "# Subscribe to the topic with the on_assign callback\n",
    "consumer.subscribe(['topic-a'], on_assign=on_assign)\n",
    "\n",
    "# Start consumer in a background thread\n",
    "consumer_thread = threading.Thread(target=consume_messages)\n",
    "consumer_thread.start()\n",
    "print(\"Consumer started\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c94a438-089a-493b-8430-17eaa5b92333",
   "metadata": {},
   "source": [
    "#### Producing Messages\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0b85ef53-1f58-4aac-999c-fe2944c33cef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sent: Text message number 0\n",
      "Received message: Text message number 0\n",
      "Sent: Text message number 1\n",
      "Received message: Text message number 1\n",
      "Sent: Text message number 2\n",
      "Received message: Text message number 2\n",
      "Sent: Text message number 3\n",
      "Received message: Text message number 3\n",
      "Sent: Text message number 4\n",
      "Received message: Text message number 4\n",
      "Sent: Text message number 5\n",
      "Received message: Text message number 5\n",
      "Sent: Text message number 6\n",
      "Received message: Text message number 6\n",
      "Sent: Text message number 7\n",
      "Received message: Text message number 7\n",
      "Sent: Text message number 8\n",
      "Received message: Text message number 8\n",
      "Sent: Text message number 9\n",
      "Received message: Text message number 9\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Producer setup\n",
    "producer = create_producer(kafka_brokers)\n",
    "\n",
    "for i in range(10):\n",
    "    producer.produce('topic-a', value=f'Text message number {i}')\n",
    "    print(f'Sent: Text message number {i}')\n",
    "    time.sleep(1)\n",
    "\n",
    "# Wait for any outstanding messages to be delivered and delivery report callbacks to be triggered\n",
    "producer.flush()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "619861cd-fcea-474f-a337-a3b6e065e9dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Messages received:\n",
      "Text message number 0\n",
      "Text message number 1\n",
      "Text message number 2\n",
      "Text message number 3\n",
      "Text message number 4\n",
      "Text message number 5\n",
      "Text message number 6\n",
      "Text message number 7\n",
      "Text message number 8\n",
      "Text message number 9\n"
     ]
    }
   ],
   "source": [
    "# Stop the consumer thread\n",
    "consumer_thread.join(timeout=1)\n",
    "\n",
    "# Display the messages received\n",
    "print(\"Messages received:\")\n",
    "for message in messages:\n",
    "    print(message)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0701388-0de3-4865-8743-56f9c12ec1ff",
   "metadata": {},
   "source": [
    "## Exercise 2: Working with Partitions\n",
    "\n",
    "### Objective\n",
    "Learn how to produce messages to specific partitions and consume them from individual partitions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b64dc638-8f88-4c48-8da5-22c99ba52f57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Producing messages to specific partitions\n",
    "topic_nam = \"topic-b\"\n",
    "producer_2 = create_producer(kafka_brokers)\n",
    "\n",
    "for i in range(10):\n",
    "    partition = i % 3  # in our case we have 3 partitions\n",
    "    message = f\"Partition {partition} - Message {i}\"\n",
    "    print(partition)\n",
    "    producer_2.produce(topic_nam, message.encode('utf-8'), partition=partition, callback=delivery_report)\n",
    "\n",
    "producer_2.flush()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1165147a-06fb-4480-ae6a-b490d608ff32",
   "metadata": {},
   "outputs": [],
   "source": [
    "from confluent_kafka import TopicPartition, KafkaException\n",
    "\n",
    "def consume_from_partition(consumer, partition_id, num_messages=5):\n",
    "    consumer.assign([TopicPartition(topic_nam, partition_id)])\n",
    "    try:\n",
    "        for _ in range(num_messages):\n",
    "            msg = consumer.poll(1.0)\n",
    "            if msg is None:\n",
    "                continue\n",
    "            if msg.error():\n",
    "                raise KafkaException(msg.error())\n",
    "            else:\n",
    "                print(f\"Received message from partition {partition_id}: {msg.value().decode('utf-8')}\")\n",
    "    except KafkaException as e:\n",
    "        print(f\"Error while consuming from partition {partition_id}: {e}\")\n",
    "    finally:\n",
    "        # Reset consumer assignment rather than closing it\n",
    "        consumer.unassign()\n",
    "\n",
    "# Creating the consumer\n",
    "consumer_2 = create_consumer(kafka_brokers, \"mygroup\")\n",
    "\n",
    "# Consuming from partition 0\n",
    "consume_from_partition(consumer_2, 0)\n",
    "\n",
    "# Consuming from partition 1\n",
    "consume_from_partition(consumer_2, 1)\n",
    "\n",
    "# Consuming from partition 1\n",
    "consume_from_partition(consumer_2, 2)\n",
    "\n",
    "# Close the consumer after all partitions have been consumed\n",
    "consumer_2.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c720e5e-0d43-4d6f-ba4b-bcc34a4370b9",
   "metadata": {},
   "source": [
    "## Exercise 3: Message Key-Based Routing\n",
    "\n",
    "### Objective\n",
    "Explore how message keys influence the partition to which a message is sent.\n",
    "\n",
    "![Keys](images/keys.png)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76c2a40e-5ef3-4da0-b987-51c49d1fdc17",
   "metadata": {},
   "outputs": [],
   "source": [
    "consumer_3 = create_consumer(kafka_brokers, \"mygroup\")\n",
    "producer_3 = create_producer(kafka_brokers)\n",
    "# Producing messages with keys\n",
    "for i in range(10):\n",
    "    key = f\"key-{i % 2}\"  # Two keys for demonstration\n",
    "    message = f\"Message with key {key}\"\n",
    "    producer_3.produce(topic_c, key=key.encode('utf-8'), value=message.encode('utf-8'), callback=delivery_report)\n",
    "\n",
    "producer_3.flush()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08626917-9109-409f-bf00-53f8482ac205",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Consuming messages and displaying keys\n",
    "consumer_3.subscribe([topic_c])\n",
    "try:\n",
    "    for _ in range(10):\n",
    "        msg = consumer_3.poll(1.0)\n",
    "        if msg is None:\n",
    "            continue\n",
    "        if msg.error():\n",
    "            raise KafkaException(msg.error())\n",
    "        else:\n",
    "            print(f\"Received message with key {msg.key().decode('utf-8')}: {msg.value().decode('utf-8')}\")\n",
    "finally:\n",
    "    consumer_3.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89666ba3-5b06-4b6a-9981-e4e8c63c634e",
   "metadata": {},
   "source": [
    "## Quiz time\n",
    "\n",
    "### As a summary of everything so far, we have prepared a short quiz\n",
    "\n",
    "Go to this [link] for the quiz. (https://take.quiz-maker.com/Q01Y2VNTR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b09c7859-079d-4ee2-ad05-2157c3fbc864",
   "metadata": {},
   "source": [
    "## Additional Resources\n",
    "\n",
    "For further learning and exploration, check out the resources used for this workshop:\n",
    "\n",
    "- [Apache Kafka Official Site](https://kafka.apache.org/)\n",
    "- [Kafka Docker Repository by Wurstmeister](https://github.com/wurstmeister/kafka-docker)\n",
    "- [Kafka Documentation](https://kafka.apache.org/documentation/)\n",
    "- [Confluent Developer Courses](https://developer.confluent.io/courses/)\n",
    "- [CloudKarafka: Kafka for Beginners](https://www.cloudkarafka.com/blog/part1-kafka-for-beginners-what-is-apache-kafka.html)\n",
    "- [Apache Kafka Introduction - WayToEasyLearn](https://waytoeasylearn.com/learn/apache-kafka-introduction/)\n",
    "- [Apache Kafka Architecture - Project Pro](https://www.projectpro.io/article/apache-kafka-architecture-/442)\n",
    "- [Confluent: Kafka Introduction](https://docs.confluent.io/kafka/introduction.html)\n",
    "- [The Evolution of Kafka Architecture: From Zookeeper to KRaft](https://romanglushach.medium.com/the-evolution-of-kafka-architecture-from-zookeeper-to-kraft-f42d511ba242)\n",
    "- [Docker Kafka Kraft on GitHub](https://github.com/moeenz/docker-kafka-kraft)\n",
    "- [Apache Kafka Architecture - JavaTpoint](https://www.javatpoint.com/apache-kafka-architecture)\n",
    "- [Apache Kafka Architecture - Instaclustr](https://www.instaclustr.com/blog/apache-kafka-architecture/)\n",
    "- [Apache Kafka GitHub Repository](https://github.com/apache/kafka)\n",
    "- [KRaft - Confluent](https://developer.confluent.io/learn/kraft/)\n",
    "- [Event Streaming](https://www.tibco.com/reference-center/what-is-event-streaming)\n",
    "- [Thank you picture](https://students.ubc.ca/ubclife/develop-deliver-riveting-presentation)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99028998-8910-4cc4-94a1-9ca918b623b6",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "Thank you for participating in the Apache Kafka workshop. We hope this session provided valuable insights into Kafka and its capabilities. Feel free to explore the additional resources and apply the knowledge in your projects.\n",
    "\n",
    "![Thank You](images/thank_you.jpg)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5691058-fe89-478d-8cb4-f2d4160b5124",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
